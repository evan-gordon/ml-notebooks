{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spooky Author Predictions Attempt Three\n",
    "\n",
    "#### Evan Gordon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/naazarik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/naazarik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "{'HPL': 1, 'MWS': 2, 'EAP': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   author_id  \n",
       "0          0  \n",
       "1          1  \n",
       "2          0  \n",
       "3          2  \n",
       "4          1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "dframe = pd.read_csv(\"../input/train.csv\")# read data into a dataframe\n",
    "authors = dict([(auth, idx) for idx, auth in enumerate(dframe['author'].unique())])#turn author names into IDs\n",
    "print(authors)\n",
    "dframe['author_id'] = dframe['author'].apply(lambda x: authors[x])\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def prepare_data(df):# Use the text and author_id fields to train a classifier.\n",
    "    sents = df['text'].tolist()#Get the sentences, \n",
    "    labels = df['author_id'].tolist()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)# Tokenize each sentence, \n",
    "    tokenizer.fit_on_texts(sents)\n",
    "    sequences = tokenizer.texts_to_sequences(sents)\n",
    "    print(len(sequences))\n",
    "    print(sequences[0])\n",
    "    \n",
    "    #print(len(sents))\n",
    "    #modified = []\n",
    "    #for sent in sents:\n",
    "    #    #modified_sent = []\n",
    "    #    for term in sent:#word_tokenize(sent):\n",
    "    #        if term in stops:\n",
    "    #            sents.remove(sent, term)\n",
    "    #            #modified_sent.append(term.lower())\n",
    "    #    modified.append(modified_sent)#======stopword end\n",
    "    #tok = Tokenizer(num_words=MAX_NB_WORDS, lower=False)\n",
    "    #tok.fit_on_texts(modified)\n",
    "    \n",
    "    \n",
    "    word_index = dict([(w,i) for w,i in tokenizer.word_index.items() if w not in stops])##    Get a vector of unique terms here\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    \n",
    "    #labels = df['author_id'].tolist()\n",
    "    #labeled_corpus = list(zip(modified, labels))\n",
    "    #print(labeled_corpus[0])\n",
    "    #print(type(labeled_corpus))\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    indices = np.arange(data.shape[0])# split the data into a training set and a validation set\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "    return data[:-num_validation_samples], labels[:-num_validation_samples], data[-num_validation_samples:], labels[-num_validation_samples:], word_index\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def print_data(testx, testy, m):#m for model\n",
    "    score = m.evaluate(testx, testy, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])#print accuracy    \n",
    "    predictions = m.predict_classes(testx)\n",
    "    Y_t = np.argmax(testy, axis=1)\n",
    "    print(precision_recall_fscore_support(Y_t, predictions))\n",
    "    print(\"Classification\")\n",
    "    print(classification_report(Y_t, predictions))\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(Y_t, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "[26, 2945, 143, 1372, 22, 36, 294, 2, 7451, 1, 2440, 2, 10, 4556, 16, 6, 79, 179, 48, 4245, 3, 295, 4, 1, 249, 1943, 6, 326, 74, 134, 123, 891, 2, 1, 313, 39, 1438, 4928, 98, 1, 430]\n",
      "Found 25808 unique tokens.\n",
      "Shape of data tensor: (19579, 1000)\n",
      "Shape of label tensor: (19579, 3)\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#  3. Load embeddings\n",
    "x_train, y_train, x_val, y_val, word_index = prepare_data(dframe)\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('../input/', 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "#  4. Create the Embedding matrix for the training set\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,245,123\n",
      "Trainable params: 245,123\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 235s 15ms/step - loss: 0.9780 - acc: 0.5196 - val_loss: 0.8929 - val_acc: 0.5900\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 238s 15ms/step - loss: 0.8147 - acc: 0.6427 - val_loss: 0.8267 - val_acc: 0.6355\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 242s 15ms/step - loss: 0.7214 - acc: 0.6915 - val_loss: 0.7682 - val_acc: 0.6613\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 232s 15ms/step - loss: 0.6287 - acc: 0.7382 - val_loss: 0.7506 - val_acc: 0.6761\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 233s 15ms/step - loss: 0.5550 - acc: 0.7732 - val_loss: 0.7585 - val_acc: 0.6822\n",
      "Test loss: 0.758516957355\n",
      "Test accuracy: 0.682247765067\n",
      "(array([ 0.64885088,  0.72267206,  0.70359848]), array([ 0.78831169,  0.6045724 ,  0.62227806]), array([ 0.71181472,  0.65836791,  0.66044444]), array([1540, 1181, 1194]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.79      0.71      1540\n",
      "          1       0.72      0.60      0.66      1181\n",
      "          2       0.70      0.62      0.66      1194\n",
      "\n",
      "avg / total       0.69      0.68      0.68      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1214  154  172]\n",
      " [ 326  714  141]\n",
      " [ 331  120  743]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 5\n",
    "pool_size = 5\n",
    "#add drop probability?\n",
    "#add hidden size?\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model1.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model1.add(MaxPooling1D(pool_size))\n",
    "model1.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model1.add(MaxPooling1D(pool_size))\n",
    "model1.add(Conv1D(conv_depth_3, k_size, activation='relu'))\n",
    "model1.add(GlobalMaxPooling1D())\n",
    "model1.add(Dense(nn_depth_1, activation='relu'))\n",
    "model1.add(Dense(len(authors), activation='softmax'))\n",
    "model1.summary()\n",
    "#model1 = Model(sequence_input, x)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model1.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My first observation of this model would be that it was the fastest to converge towards it's optimal state. No matter what changes i made in my previous models none of them converged nearly this fast reaching over 70% accuracy in just over 3 epochs.\n",
    "Since the shape of this model is based off of the one provided my next goal is to take my best model from my second notebook and try training that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,163,075\n",
      "Trainable params: 163,075\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 246s 16ms/step - loss: 0.9703 - acc: 0.5328 - val_loss: 0.9024 - val_acc: 0.5780\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 246s 16ms/step - loss: 0.8073 - acc: 0.6419 - val_loss: 0.8085 - val_acc: 0.6365\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 246s 16ms/step - loss: 0.7173 - acc: 0.6894 - val_loss: 0.7772 - val_acc: 0.6508\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 246s 16ms/step - loss: 0.6230 - acc: 0.7438 - val_loss: 0.9404 - val_acc: 0.5900\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 246s 16ms/step - loss: 0.5511 - acc: 0.7774 - val_loss: 0.7506 - val_acc: 0.6812\n",
      "Test loss: 0.750607401171\n",
      "Test accuracy: 0.681226053625\n",
      "(array([ 0.72389629,  0.66923736,  0.64572294]), array([ 0.67077922,  0.66130398,  0.71440536]), array([ 0.69632626,  0.66524702,  0.67833002]), array([1540, 1181, 1194]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.67      0.70      1540\n",
      "          1       0.67      0.66      0.67      1181\n",
      "          2       0.65      0.71      0.68      1194\n",
      "\n",
      "avg / total       0.68      0.68      0.68      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1033  238  269]\n",
      " [ 201  781  199]\n",
      " [ 193  148  853]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 5\n",
    "pool_size = 5\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model2.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model2.add(MaxPooling1D(pool_size))\n",
    "model2.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(nn_depth_1, activation='relu'))\n",
    "model2.add(Dense(len(authors), activation='softmax'))\n",
    "model2.summary()\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model2.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 998, 128)          38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,104,707\n",
      "Trainable params: 104,707\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 167s 11ms/step - loss: 0.9869 - acc: 0.5175 - val_loss: 0.9801 - val_acc: 0.5504\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 166s 11ms/step - loss: 0.8269 - acc: 0.6306 - val_loss: 0.8160 - val_acc: 0.6455\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 172s 11ms/step - loss: 0.7532 - acc: 0.6687 - val_loss: 0.7833 - val_acc: 0.6529\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 172s 11ms/step - loss: 0.6807 - acc: 0.7135 - val_loss: 0.9737 - val_acc: 0.5967\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 173s 11ms/step - loss: 0.6155 - acc: 0.7434 - val_loss: 0.8424 - val_acc: 0.6365\n",
      "Test loss: 0.842387589915\n",
      "Test accuracy: 0.636526181323\n",
      "(array([ 0.74650513,  0.6798893 ,  0.54266212]), array([ 0.52012987,  0.62404742,  0.79899497]), array([ 0.6130884 ,  0.65077263,  0.64634146]), array([1540, 1181, 1194]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.52      0.61      1540\n",
      "          1       0.68      0.62      0.65      1181\n",
      "          2       0.54      0.80      0.65      1194\n",
      "\n",
      "avg / total       0.66      0.64      0.63      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[801 240 499]\n",
      " [139 737 305]\n",
      " [133 107 954]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 3\n",
    "pool_size = 5\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model2.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model2.add(MaxPooling1D(pool_size))\n",
    "model2.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(nn_depth_1, activation='relu'))\n",
    "model2.add(Dense(len(authors), activation='softmax'))\n",
    "model2.summary()\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model2.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "From looking at the accuracy of the different models that I produced here by modifying the input data and then tweaking certain values of the model itself, my second model (model2) was the most accurate thus far.\n",
    "\n",
    "In order to get a better indicator of how accurate this model can get i will now run it for 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 998, 128)          38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,104,707\n",
      "Trainable params: 104,707\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.9761 - acc: 0.5305 - val_loss: 0.9067 - val_acc: 0.5834\n",
      "Epoch 2/30\n",
      "15664/15664 [==============================] - 166s 11ms/step - loss: 0.8313 - acc: 0.6288 - val_loss: 0.8258 - val_acc: 0.6278\n",
      "Epoch 3/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.7520 - acc: 0.6747 - val_loss: 0.7796 - val_acc: 0.6498\n",
      "Epoch 4/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.6798 - acc: 0.7116 - val_loss: 0.8134 - val_acc: 0.6531\n",
      "Epoch 5/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.6286 - acc: 0.7344 - val_loss: 0.7776 - val_acc: 0.6656\n",
      "Epoch 6/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.5685 - acc: 0.7654 - val_loss: 0.7688 - val_acc: 0.6756\n",
      "Epoch 7/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.5181 - acc: 0.7846 - val_loss: 0.8476 - val_acc: 0.6669\n",
      "Epoch 8/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.4689 - acc: 0.8091 - val_loss: 0.7577 - val_acc: 0.6886\n",
      "Epoch 9/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.4360 - acc: 0.8293 - val_loss: 0.8881 - val_acc: 0.6531\n",
      "Epoch 10/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.3909 - acc: 0.8452 - val_loss: 0.8843 - val_acc: 0.6710\n",
      "Epoch 11/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.3549 - acc: 0.8620 - val_loss: 0.8377 - val_acc: 0.6937\n",
      "Epoch 12/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.3264 - acc: 0.8786 - val_loss: 1.0203 - val_acc: 0.6725\n",
      "Epoch 13/30\n",
      "15664/15664 [==============================] - 167s 11ms/step - loss: 0.3093 - acc: 0.8848 - val_loss: 0.8676 - val_acc: 0.6973\n",
      "Epoch 14/30\n",
      "15664/15664 [==============================] - 167s 11ms/step - loss: 0.2667 - acc: 0.9090 - val_loss: 1.4190 - val_acc: 0.6179\n",
      "Epoch 15/30\n",
      "15664/15664 [==============================] - 167s 11ms/step - loss: 0.2691 - acc: 0.9073 - val_loss: 0.9872 - val_acc: 0.6955\n",
      "Epoch 16/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.2426 - acc: 0.9262 - val_loss: 1.0596 - val_acc: 0.6960\n",
      "Epoch 17/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.2143 - acc: 0.9320 - val_loss: 1.0817 - val_acc: 0.6943\n",
      "Epoch 18/30\n",
      "15664/15664 [==============================] - 167s 11ms/step - loss: 0.2320 - acc: 0.9358 - val_loss: 1.1381 - val_acc: 0.6871\n",
      "Epoch 19/30\n",
      "15664/15664 [==============================] - 167s 11ms/step - loss: 0.1880 - acc: 0.9484 - val_loss: 1.2346 - val_acc: 0.6930\n",
      "Epoch 20/30\n",
      "15664/15664 [==============================] - 169s 11ms/step - loss: 0.2149 - acc: 0.9478 - val_loss: 1.2629 - val_acc: 0.6894\n",
      "Epoch 21/30\n",
      "15664/15664 [==============================] - 168s 11ms/step - loss: 0.2226 - acc: 0.9482 - val_loss: 1.3623 - val_acc: 0.6889\n",
      "Epoch 22/30\n",
      "15664/15664 [==============================] - 164s 10ms/step - loss: 0.1482 - acc: 0.9588 - val_loss: 1.7349 - val_acc: 0.6682\n",
      "Epoch 23/30\n",
      "15664/15664 [==============================] - 164s 10ms/step - loss: 0.1740 - acc: 0.9556 - val_loss: 1.3828 - val_acc: 0.6989\n",
      "Epoch 24/30\n",
      "15664/15664 [==============================] - 164s 10ms/step - loss: 0.1766 - acc: 0.9573 - val_loss: 1.4444 - val_acc: 0.6891\n",
      "Epoch 25/30\n",
      "15664/15664 [==============================] - 164s 10ms/step - loss: 0.1582 - acc: 0.9633 - val_loss: 4.7393 - val_acc: 0.4695\n",
      "Epoch 26/30\n",
      "15664/15664 [==============================] - 164s 10ms/step - loss: 0.1782 - acc: 0.9612 - val_loss: 1.6085 - val_acc: 0.6820\n",
      "Epoch 27/30\n",
      "15664/15664 [==============================] - 164s 10ms/step - loss: 0.1576 - acc: 0.9667 - val_loss: 1.5670 - val_acc: 0.6881\n",
      "Epoch 28/30\n",
      "15664/15664 [==============================] - 164s 10ms/step - loss: 0.1395 - acc: 0.9680 - val_loss: 1.6395 - val_acc: 0.6815\n",
      "Epoch 29/30\n",
      "15664/15664 [==============================] - 164s 10ms/step - loss: 0.1828 - acc: 0.9640 - val_loss: 1.6600 - val_acc: 0.6838\n",
      "Epoch 30/30\n",
      "15664/15664 [==============================] - 164s 10ms/step - loss: 0.1581 - acc: 0.9654 - val_loss: 1.6564 - val_acc: 0.6871\n",
      "Test loss: 1.65642178366\n",
      "Test accuracy: 0.687100893967\n",
      "(array([ 0.68657635,  0.68563923,  0.68929504]), array([ 0.72402597,  0.66299746,  0.66331658]), array([ 0.70480405,  0.67412828,  0.67605634]), array([1540, 1181, 1194]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.72      0.70      1540\n",
      "          1       0.69      0.66      0.67      1181\n",
      "          2       0.69      0.66      0.68      1194\n",
      "\n",
      "avg / total       0.69      0.69      0.69      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1115  215  210]\n",
      " [ 251  783  147]\n",
      " [ 258  144  792]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 3\n",
    "pool_size = 5\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model2.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model2.add(MaxPooling1D(pool_size))\n",
    "model2.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(nn_depth_1, activation='relu'))\n",
    "model2.add(Dense(len(authors), activation='softmax'))\n",
    "model2.summary()\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model2.fit(x_train, y_train, batch_size=batch_size, epochs=30, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "While this model ended up with a high accuracy of 96.5% the value accuracy turned out to not be much better than the rest of my models, having a value accuracy of 68.7%. For future tests i might concider adding in garbage words to the model, removing smaller words (since larger words tend to be used less frequently and might help indicate who is talking) and possibly try changing values in the model some more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
