{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Convolutional Neural Networks for Author Prediction\n",
    "\n",
    "Evan Gordon\n",
    "\n",
    "\n",
    "#### Base Example Model\n",
    "The point of this notebook is to experiment with tweaking the shape size and sampling features of the model without actually modifying the input data. I want to do this in hopes to find the best method i can this way, and then see how similar changes might affect my third model when i do start making changes to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MWS': 2, 'EAP': 0, 'HPL': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   author_id  \n",
       "0          0  \n",
       "1          1  \n",
       "2          0  \n",
       "3          2  \n",
       "4          1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "#from nltk.corpus import stopwords\n",
    "#import nltk\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "dframe = pd.read_csv(\"../input/train.csv\")# read data into a dataframe\n",
    "authors = dict([(auth, idx) for idx, auth in enumerate(dframe['author'].unique())])#turn author names into IDs\n",
    "print(authors)\n",
    "dframe['author_id'] = dframe['author'].apply(lambda x: authors[x])\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):# Use the text and author_id fields to train a classifier.\n",
    "    sents = df['text'].tolist()#Get the sentences, \n",
    "    labels = df['author_id'].tolist()\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)# Tokenize each sentence, \n",
    "    tokenizer.fit_on_texts(sents)\n",
    "    sequences = tokenizer.texts_to_sequences(sents)\n",
    "    print(len(sequences))\n",
    "    print(sequences[0])\n",
    "    \n",
    "    word_index = tokenizer.word_index##    Get a vector of unique terms here\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = to_categorical(np.asarray(labels))\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    indices = np.arange(data.shape[0])# split the data into a training set and a validation set\n",
    "    np.random.shuffle(indices)\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "    return data[:-num_validation_samples], labels[:-num_validation_samples], data[-num_validation_samples:], labels[-num_validation_samples:], word_index\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def print_data(testx, testy, m):#m for model\n",
    "    score = m.evaluate(testx, testy, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])#print accuracy    \n",
    "    predictions = m.predict_classes(testx)\n",
    "    Y_t = np.argmax(testy, axis=1)\n",
    "    print(precision_recall_fscore_support(Y_t, predictions))\n",
    "    print(\"Classification\")\n",
    "    print(classification_report(Y_t, predictions))\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(Y_t, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "[26, 2945, 143, 1372, 22, 36, 294, 2, 7451, 1, 2440, 2, 10, 4556, 16, 6, 79, 179, 48, 4245, 3, 295, 4, 1, 249, 1943, 6, 326, 74, 134, 123, 891, 2, 1, 313, 39, 1438, 4928, 98, 1, 430]\n",
      "<class 'list'>\n",
      "Found 25943 unique tokens.\n",
      "Shape of data tensor: (19579, 1000)\n",
      "Shape of label tensor: (19579, 3)\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#  3. Load embeddings\n",
    "x_train, y_train, x_val, y_val, word_index = prepare_data(dframe)\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('../input/', 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "#  4. Create the Embedding matrix for the training set\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_113 (Conv1D)          (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_73 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_114 (Conv1D)          (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_74 (MaxPooling (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 35, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_27 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,245,123\n",
      "Trainable params: 245,123\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 245s 16ms/step - loss: 0.9863 - acc: 0.5237 - val_loss: 0.8881 - val_acc: 0.5977\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 242s 15ms/step - loss: 0.8234 - acc: 0.6352 - val_loss: 0.7943 - val_acc: 0.6544\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 237s 15ms/step - loss: 0.7428 - acc: 0.6806 - val_loss: 0.7685 - val_acc: 0.6679\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 232s 15ms/step - loss: 0.6635 - acc: 0.7278 - val_loss: 0.7735 - val_acc: 0.6575\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 232s 15ms/step - loss: 0.5978 - acc: 0.7523 - val_loss: 0.7359 - val_acc: 0.6881\n",
      "Test loss: 0.735894044788\n",
      "Test accuracy: 0.68812260541\n",
      "(array([ 0.6329588 ,  0.74093817,  0.76932224]), array([ 0.8476489 ,  0.60911481,  0.54877014]), array([ 0.72473868,  0.66859067,  0.64059406]), array([1595, 1141, 1179]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.85      0.72      1595\n",
      "          1       0.74      0.61      0.67      1141\n",
      "          2       0.77      0.55      0.64      1179\n",
      "\n",
      "avg / total       0.71      0.69      0.68      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1352  122  121]\n",
      " [ 373  695   73]\n",
      " [ 411  121  647]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 5\n",
    "pool_size = 5\n",
    "#add drop probability?\n",
    "#add hidden size?\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model1.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model1.add(MaxPooling1D(pool_size))\n",
    "model1.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model1.add(MaxPooling1D(pool_size))\n",
    "model1.add(Conv1D(conv_depth_3, k_size, activation='relu'))\n",
    "model1.add(GlobalMaxPooling1D())\n",
    "model1.add(Dense(nn_depth_1, activation='relu'))\n",
    "model1.add(Dense(len(authors), activation='softmax'))\n",
    "model1.summary()\n",
    "#model1 = Model(sequence_input, x)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model1.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "This model is based off of Joe Dumoulin's base model at: https://github.com/JoeDumoulin/CSCD439F17/blob/master/notebooks/Final%20Project/Keras%20Convolutional%20Network%20for%20Spooky%20Author%20ID0.ipynb\n",
    "\n",
    "The resulting model came across as a bit bulky and I intend to make a model that is a bit smaller than this one and see how a smaller model with less layers might fare compared to this one. Running this model a full 50 times like Joe did is a bit impossible on my current machine so the evaluations of this model will have to be based on what I have. From here on I will split up my efforts and try to create two different models. \n",
    "\n",
    "The first step will be to try to find a better shape for the layers of the model.\n",
    "If i can find a shape that produces a more accurate prediction of the data I'll submit that model for testing.\n",
    "I'll then create another model, but this time remove the stop words. This model will use the same shape as the previous model. By doing it this way i can create a somewhat controlled environment, in which I can clearly see the effect of removing stopwords from a dataset.\n",
    "\n",
    "\n",
    "#### Model One \n",
    "\n",
    "##### attempt one: remove one convolutional layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_75 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_117 (Conv1D)          (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_28 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,163,075\n",
      "Trainable params: 163,075\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 249s 16ms/step - loss: 1.0038 - acc: 0.5144 - val_loss: 0.8810 - val_acc: 0.6051\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 247s 16ms/step - loss: 0.8290 - acc: 0.6307 - val_loss: 0.7824 - val_acc: 0.6554\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 250s 16ms/step - loss: 0.7478 - acc: 0.6766 - val_loss: 0.7488 - val_acc: 0.6774\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 251s 16ms/step - loss: 0.6686 - acc: 0.7186 - val_loss: 0.7099 - val_acc: 0.7034\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 248s 16ms/step - loss: 0.5978 - acc: 0.7526 - val_loss: 0.7216 - val_acc: 0.6960\n",
      "Test loss: 0.721583791117\n",
      "Test accuracy: 0.696040868439\n",
      "(array([ 0.70983506,  0.80051151,  0.6263369 ]), array([ 0.72852665,  0.54864154,  0.79474131]), array([ 0.71905941,  0.65106604,  0.70056075]), array([1595, 1141, 1179]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.73      0.72      1595\n",
      "          1       0.80      0.55      0.65      1141\n",
      "          2       0.63      0.79      0.70      1179\n",
      "\n",
      "avg / total       0.71      0.70      0.69      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1162  105  328]\n",
      " [ 284  626  231]\n",
      " [ 191   51  937]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 5\n",
    "pool_size = 5\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model2.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model2.add(MaxPooling1D(pool_size))\n",
    "model2.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(nn_depth_1, activation='relu'))\n",
    "model2.add(Dense(len(authors), activation='softmax'))\n",
    "model2.summary()\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model2.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall given the 5 epocs I ran this model ended up slightly more accurate than the base model (though only marginally so) I would hypothesize that this might not be the case if more epochs were to be run. To this end I'm going to try a compromise, I'm going to add the 3rd layer back into the model, however this time I will be decreasing the size of that layer and the Dense layer by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_118 (Conv1D)          (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_76 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_119 (Conv1D)          (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_77 (MaxPooling (None, 39, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_120 (Conv1D)          (None, 35, 64)            41024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_29 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 2,191,555\n",
      "Trainable params: 191,555\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 230s 15ms/step - loss: 0.9960 - acc: 0.5015 - val_loss: 0.9488 - val_acc: 0.5285\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 223s 14ms/step - loss: 0.8267 - acc: 0.6350 - val_loss: 0.7961 - val_acc: 0.6485\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 226s 14ms/step - loss: 0.7512 - acc: 0.6785 - val_loss: 0.9202 - val_acc: 0.5895\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 228s 15ms/step - loss: 0.6808 - acc: 0.7130 - val_loss: 0.7563 - val_acc: 0.6741\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 226s 14ms/step - loss: 0.6048 - acc: 0.7526 - val_loss: 0.7072 - val_acc: 0.6943\n",
      "Test loss: 0.707237766194\n",
      "Test accuracy: 0.694252873609\n",
      "(array([ 0.72207959,  0.63420074,  0.7312253 ]), array([ 0.70532915,  0.74758983,  0.62765055]), array([ 0.71360609,  0.68624296,  0.67549064]), array([1595, 1141, 1179]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.71      0.71      1595\n",
      "          1       0.63      0.75      0.69      1141\n",
      "          2       0.73      0.63      0.68      1179\n",
      "\n",
      "avg / total       0.70      0.69      0.69      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1125  288  182]\n",
      " [ 198  853   90]\n",
      " [ 235  204  740]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 64\n",
    "nn_depth_1 = 64\n",
    "k_size = 5\n",
    "pool_size = 5\n",
    "#add drop probability?\n",
    "#add hidden size?\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model3.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model3.add(MaxPooling1D(pool_size))\n",
    "model3.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model3.add(MaxPooling1D(pool_size))\n",
    "model3.add(Conv1D(conv_depth_3, k_size, activation='relu'))\n",
    "model3.add(GlobalMaxPooling1D())\n",
    "model3.add(Dense(nn_depth_1, activation='relu'))\n",
    "model3.add(Dense(len(authors), activation='softmax'))\n",
    "model3.summary()\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model3.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was ok, but the recall ended up worse. Otherwise this model ended up producing almost identical results, thus i think this model in the long run would end up inferior so I won't end up continuing donw this path. I'm going to roll back to the previous model but now try tweaking the kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_121 (Conv1D)          (None, 994, 128)          89728     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_78 (MaxPooling (None, 198, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_122 (Conv1D)          (None, 192, 128)          114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_30 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,221,443\n",
      "Trainable params: 221,443\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 332s 21ms/step - loss: 1.0623 - acc: 0.4735 - val_loss: 0.9188 - val_acc: 0.5640\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 330s 21ms/step - loss: 0.9092 - acc: 0.5831 - val_loss: 0.8750 - val_acc: 0.5951\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 330s 21ms/step - loss: 0.8342 - acc: 0.6294 - val_loss: 0.8063 - val_acc: 0.6460\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 332s 21ms/step - loss: 0.7879 - acc: 0.6532 - val_loss: 0.8024 - val_acc: 0.6332\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 328s 21ms/step - loss: 0.7447 - acc: 0.6798 - val_loss: 0.9636 - val_acc: 0.5722\n",
      "Test loss: 0.96362528466\n",
      "Test accuracy: 0.5721583653\n",
      "(array([ 0.73943662,  0.53312102,  0.53700306]), array([ 0.32915361,  0.73356705,  0.7446989 ]), array([ 0.45553145,  0.61748432,  0.62402274]), array([1595, 1141, 1179]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.33      0.46      1595\n",
      "          1       0.53      0.73      0.62      1141\n",
      "          2       0.54      0.74      0.62      1179\n",
      "\n",
      "avg / total       0.62      0.57      0.55      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[525 528 542]\n",
      " [ 89 837 215]\n",
      " [ 96 205 878]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 7\n",
    "pool_size = 5\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model4.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model4.add(MaxPooling1D(pool_size))\n",
    "model4.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model4.add(GlobalMaxPooling1D())\n",
    "model4.add(Dense(nn_depth_1, activation='relu'))\n",
    "model4.add(Dense(len(authors), activation='softmax'))\n",
    "model4.summary()\n",
    "model4.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model4.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I learned here is that increasing the kernel size didn't really help my model in a positive way. I'm going to now revert back to a kernel size of 5 but this time try increasing the size of the first convolutional layer, and then add some dropout to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_123 (Conv1D)          (None, 996, 256)          128256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_79 (MaxPooling (None, 199, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 199, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_124 (Conv1D)          (None, 195, 128)          163968    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_31 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,309,123\n",
      "Trainable params: 309,123\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 458s 29ms/step - loss: 1.1433 - acc: 0.4085 - val_loss: 1.0412 - val_acc: 0.4513\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 453s 29ms/step - loss: 1.0258 - acc: 0.4930 - val_loss: 0.9584 - val_acc: 0.5752\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 459s 29ms/step - loss: 0.9634 - acc: 0.5529 - val_loss: 0.9194 - val_acc: 0.5997\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 457s 29ms/step - loss: 0.9082 - acc: 0.5879 - val_loss: 0.8755 - val_acc: 0.6396\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 458s 29ms/step - loss: 0.8704 - acc: 0.6113 - val_loss: 0.8462 - val_acc: 0.6375\n",
      "Test loss: 0.846203564014\n",
      "Test accuracy: 0.637547892758\n",
      "(array([ 0.59102675,  0.68220339,  0.7381317 ]), array([ 0.85893417,  0.56441718,  0.40882103]), array([ 0.70023   ,  0.6177458 ,  0.52620087]), array([1595, 1141, 1179]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.86      0.70      1595\n",
      "          1       0.68      0.56      0.62      1141\n",
      "          2       0.74      0.41      0.53      1179\n",
      "\n",
      "avg / total       0.66      0.64      0.62      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1370  122  103]\n",
      " [ 429  644   68]\n",
      " [ 519  178  482]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 256\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 5\n",
    "pool_size = 5\n",
    "drop_prob_1 = 0.5\n",
    "#add hidden size?\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model5.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model5.add(MaxPooling1D(pool_size))\n",
    "model5.add(Dropout(drop_prob_1))\n",
    "model5.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model5.add(GlobalMaxPooling1D())\n",
    "model5.add(Dropout(drop_prob_1))\n",
    "model5.add(Dense(nn_depth_1, activation='relu'))\n",
    "model5.add(Dense(len(authors), activation='softmax'))\n",
    "model5.summary()\n",
    "model5.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model5.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_127 (Conv1D)          (None, 998, 128)          38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_81 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_128 (Conv1D)          (None, 197, 128)          49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_33 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,104,707\n",
      "Trainable params: 104,707\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 176s 11ms/step - loss: 1.0655 - acc: 0.4563 - val_loss: 0.9759 - val_acc: 0.5167\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 175s 11ms/step - loss: 0.9292 - acc: 0.5668 - val_loss: 0.8592 - val_acc: 0.6281\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 174s 11ms/step - loss: 0.8594 - acc: 0.6133 - val_loss: 0.8046 - val_acc: 0.6526\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 175s 11ms/step - loss: 0.8163 - acc: 0.6375 - val_loss: 0.7891 - val_acc: 0.6488\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 176s 11ms/step - loss: 0.7796 - acc: 0.6627 - val_loss: 0.7807 - val_acc: 0.6575\n",
      "Test loss: 0.780709578693\n",
      "Test accuracy: 0.657471264413\n",
      "(array([ 0.71752577,  0.7785059 ,  0.55156158]), array([ 0.65454545,  0.52059597,  0.79389313]), array([ 0.68459016,  0.62394958,  0.65090403]), array([1595, 1141, 1179]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.65      0.68      1595\n",
      "          1       0.78      0.52      0.62      1141\n",
      "          2       0.55      0.79      0.65      1179\n",
      "\n",
      "avg / total       0.69      0.66      0.66      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1044  119  432]\n",
      " [ 218  594  329]\n",
      " [ 193   50  936]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 3\n",
    "pool_size = 5\n",
    "drop_prob_1 = 0.5\n",
    "\n",
    "model6 = Sequential()\n",
    "model6.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model6.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model6.add(MaxPooling1D(pool_size))\n",
    "model6.add(Dropout(drop_prob_1))\n",
    "model6.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model6.add(GlobalMaxPooling1D())\n",
    "model6.add(Dropout(drop_prob_1))\n",
    "model6.add(Dense(nn_depth_1, activation='relu'))\n",
    "model6.add(Dense(len(authors), activation='softmax'))\n",
    "model6.summary()\n",
    "model6.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model6.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet again this model hasn't suprassed my original modification from the main model. I'm going to make one more attempt. In this one I'm reverting to that original modification and adding in the dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_129 (Conv1D)          (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_82 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_130 (Conv1D)          (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_34 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,163,075\n",
      "Trainable params: 163,075\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/5\n",
      "15664/15664 [==============================] - 256s 16ms/step - loss: 1.0546 - acc: 0.4667 - val_loss: 0.9148 - val_acc: 0.5982\n",
      "Epoch 2/5\n",
      "15664/15664 [==============================] - 250s 16ms/step - loss: 0.9151 - acc: 0.5798 - val_loss: 0.8406 - val_acc: 0.6406\n",
      "Epoch 3/5\n",
      "15664/15664 [==============================] - 250s 16ms/step - loss: 0.8480 - acc: 0.6242 - val_loss: 0.7800 - val_acc: 0.6656\n",
      "Epoch 4/5\n",
      "15664/15664 [==============================] - 249s 16ms/step - loss: 0.8020 - acc: 0.6491 - val_loss: 0.7660 - val_acc: 0.6687\n",
      "Epoch 5/5\n",
      "15664/15664 [==============================] - 259s 17ms/step - loss: 0.7626 - acc: 0.6691 - val_loss: 0.8097 - val_acc: 0.6322\n",
      "Test loss: 0.80974376328\n",
      "Test accuracy: 0.632183908084\n",
      "(array([ 0.66969869,  0.53321364,  0.8371134 ]), array([ 0.73855799,  0.78089395,  0.34435963]), array([ 0.70244484,  0.63371266,  0.48798077]), array([1595, 1141, 1179]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.74      0.70      1595\n",
      "          1       0.53      0.78      0.63      1141\n",
      "          2       0.84      0.34      0.49      1179\n",
      "\n",
      "avg / total       0.68      0.63      0.62      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1178  362   55]\n",
      " [ 226  891   24]\n",
      " [ 355  418  406]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 5\n",
    "pool_size = 5\n",
    "drop_prob_1 = 0.5\n",
    "\n",
    "model7 = Sequential()\n",
    "model7.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model7.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model7.add(MaxPooling1D(pool_size))\n",
    "model7.add(Dropout(drop_prob_1))\n",
    "model7.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model7.add(GlobalMaxPooling1D())\n",
    "model7.add(Dropout(drop_prob_1))\n",
    "model7.add(Dense(nn_depth_1, activation='relu'))\n",
    "model7.add(Dense(len(authors), activation='softmax'))\n",
    "model7.summary()\n",
    "model7.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model7.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like between all of my different attempts removing the third convolutional layer seems to have accected the model the most positively, so since none of my other attepmts produced noticable improved results within the given timeframe of 5 epochs, I am going to now run my best model but for 25 epochs and see about where the accuracy of my model ends up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 1000, 100)         2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_131 (Conv1D)          (None, 996, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_83 (MaxPooling (None, 199, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_132 (Conv1D)          (None, 195, 128)          82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_35 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,163,075\n",
      "Trainable params: 163,075\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Training...\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/25\n",
      "15664/15664 [==============================] - 249s 16ms/step - loss: 1.0122 - acc: 0.4997 - val_loss: 0.8611 - val_acc: 0.6266\n",
      "Epoch 2/25\n",
      "15664/15664 [==============================] - 248s 16ms/step - loss: 0.8318 - acc: 0.6256 - val_loss: 0.8919 - val_acc: 0.5987\n",
      "Epoch 3/25\n",
      "15664/15664 [==============================] - 250s 16ms/step - loss: 0.7468 - acc: 0.6796 - val_loss: 0.7716 - val_acc: 0.6580\n",
      "Epoch 4/25\n",
      "15664/15664 [==============================] - 252s 16ms/step - loss: 0.6705 - acc: 0.7145 - val_loss: 0.7248 - val_acc: 0.6848\n",
      "Epoch 5/25\n",
      "15664/15664 [==============================] - 251s 16ms/step - loss: 0.6094 - acc: 0.7462 - val_loss: 0.6891 - val_acc: 0.7063\n",
      "Epoch 6/25\n",
      "15664/15664 [==============================] - 251s 16ms/step - loss: 0.5356 - acc: 0.7821 - val_loss: 0.7902 - val_acc: 0.6835\n",
      "Epoch 7/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.4715 - acc: 0.8109 - val_loss: 1.2342 - val_acc: 0.5765\n",
      "Epoch 8/25\n",
      "15664/15664 [==============================] - 252s 16ms/step - loss: 0.4095 - acc: 0.8429 - val_loss: 0.7494 - val_acc: 0.7040\n",
      "Epoch 9/25\n",
      "15664/15664 [==============================] - 252s 16ms/step - loss: 0.3465 - acc: 0.8647 - val_loss: 0.9078 - val_acc: 0.6774\n",
      "Epoch 10/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.2985 - acc: 0.8887 - val_loss: 0.9727 - val_acc: 0.6930\n",
      "Epoch 11/25\n",
      "15664/15664 [==============================] - 252s 16ms/step - loss: 0.2732 - acc: 0.9070 - val_loss: 0.9958 - val_acc: 0.6907\n",
      "Epoch 12/25\n",
      "15664/15664 [==============================] - 252s 16ms/step - loss: 0.2352 - acc: 0.9218 - val_loss: 1.2395 - val_acc: 0.6391\n",
      "Epoch 13/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.2120 - acc: 0.9353 - val_loss: 1.3671 - val_acc: 0.6567\n",
      "Epoch 14/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.1857 - acc: 0.9484 - val_loss: 1.3152 - val_acc: 0.6771\n",
      "Epoch 15/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.1835 - acc: 0.9526 - val_loss: 1.6023 - val_acc: 0.6605\n",
      "Epoch 16/25\n",
      "15664/15664 [==============================] - 254s 16ms/step - loss: 0.1820 - acc: 0.9556 - val_loss: 1.1312 - val_acc: 0.6966\n",
      "Epoch 17/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.1571 - acc: 0.9623 - val_loss: 1.1678 - val_acc: 0.6978\n",
      "Epoch 18/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.1394 - acc: 0.9729 - val_loss: 1.4140 - val_acc: 0.7009\n",
      "Epoch 19/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.1647 - acc: 0.9646 - val_loss: 1.3251 - val_acc: 0.6978\n",
      "Epoch 20/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.1414 - acc: 0.9728 - val_loss: 1.4149 - val_acc: 0.6994\n",
      "Epoch 21/25\n",
      "15664/15664 [==============================] - 254s 16ms/step - loss: 0.1472 - acc: 0.9721 - val_loss: 1.4480 - val_acc: 0.6912\n",
      "Epoch 22/25\n",
      "15664/15664 [==============================] - 255s 16ms/step - loss: 0.1235 - acc: 0.9771 - val_loss: 1.6061 - val_acc: 0.6935\n",
      "Epoch 23/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.1337 - acc: 0.9746 - val_loss: 2.2504 - val_acc: 0.6238\n",
      "Epoch 24/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.1190 - acc: 0.9747 - val_loss: 1.6180 - val_acc: 0.6891\n",
      "Epoch 25/25\n",
      "15664/15664 [==============================] - 253s 16ms/step - loss: 0.1100 - acc: 0.9787 - val_loss: 1.6642 - val_acc: 0.6807\n",
      "Test loss: 1.66418826367\n",
      "Test accuracy: 0.680715198017\n",
      "(array([ 0.73052632,  0.73229167,  0.60196078]), array([ 0.65266458,  0.61612621,  0.78117048]), array([ 0.68940397,  0.66920514,  0.6799557 ]), array([1595, 1141, 1179]))\n",
      "Classification\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.65      0.69      1595\n",
      "          1       0.73      0.62      0.67      1141\n",
      "          2       0.60      0.78      0.68      1179\n",
      "\n",
      "avg / total       0.69      0.68      0.68      3915\n",
      "\n",
      "Confusion Matrix\n",
      "[[1041  165  389]\n",
      " [ 218  703  220]\n",
      " [ 166   92  921]]\n"
     ]
    }
   ],
   "source": [
    "#model 2 revisited\n",
    "from keras.models import Sequential\n",
    "batch_size = 128\n",
    "num_epochs = 25\n",
    "conv_depth_1 = 128\n",
    "conv_depth_2 = 128\n",
    "conv_depth_3 = 128\n",
    "nn_depth_1 = 128\n",
    "k_size = 5\n",
    "pool_size = 5\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "model2.add(Conv1D(conv_depth_1, k_size, activation='relu', input_shape =(1000, 1)))\n",
    "model2.add(MaxPooling1D(pool_size))\n",
    "model2.add(Conv1D(conv_depth_2, k_size, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(nn_depth_1, activation='relu'))\n",
    "model2.add(Dense(len(authors), activation='softmax'))\n",
    "model2.summary()\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Training...\")\n",
    "model2.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs, verbose=1, validation_data=(x_val, y_val))\n",
    "\n",
    "print_data(x_val, y_val, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result of the Model\n",
    "The accuracy of this model after going through the full 25 epochs turned out to be quite high. By human standards this would be a very accurate model even with how low the value accuracy is.\n",
    "\n",
    "Please see the next notbook (final3) for a futher attempt at this subject."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
