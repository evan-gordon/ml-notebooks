{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Horror Authors Using Naive Bayes\n",
    "\n",
    "#### Evan Gordon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/naazarik/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/naazarik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "(19579, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "df = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for preparing data\n",
    "from nltk.tokenize import word_tokenize\n",
    "def prepare_data(dataframe):\n",
    "    corpus = dataframe['text'].tolist()# Read the text of the training examples  \n",
    "    unique_labels = dataframe['author'].unique().tolist()\n",
    "    word_tokenize(corpus[0])#tokenize\n",
    "    stops = set(stopwords.words('english'))# Remove english stopwords from the tokenized lists\n",
    "    modified_corpus = []\n",
    "    for sent in corpus:\n",
    "        modified_sent = []\n",
    "        for term in word_tokenize(sent):\n",
    "            if term not in stops:\n",
    "                modified_sent.append(term)\n",
    "        modified_corpus.append(modified_sent)\n",
    "    #print(modified_corpus[0])#print without stopwords\n",
    "    labels = dataframe['author'].tolist()\n",
    "    print(unique_labels)\n",
    "    labeled_corpus = list(zip(modified_corpus, labels))#label data\n",
    "    print(labeled_corpus[0])#print without stopwords, but with labels\n",
    "    return labeled_corpus\n",
    "\n",
    "def add_class(prepared_data):\n",
    "    data = []\n",
    "    for passage in prepared_data:\n",
    "        d = {}\n",
    "        for term in passage[0]:\n",
    "            d[term] = True\n",
    "        data.append((d, passage[1]))\n",
    "    print(data[0])\n",
    "    print()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP', 'HPL', 'MWS']\n",
      "(['This', 'process', ',', 'however', ',', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', ';', 'I', 'might', 'make', 'circuit', ',', 'return', 'point', 'whence', 'I', 'set', ',', 'without', 'aware', 'fact', ';', 'perfectly', 'uniform', 'seemed', 'wall', '.'], 'EAP')\n",
      "({'afforded': True, 'ascertaining': True, 'whence': True, 'This': True, 'might': True, 'I': True, 'however': True, 'point': True, '.': True, ';': True, 'seemed': True, 'make': True, 'circuit': True, ',': True, 'dungeon': True, 'means': True, 'without': True, 'uniform': True, 'dimensions': True, 'aware': True, 'fact': True, 'wall': True, 'return': True, 'set': True, 'perfectly': True, 'process': True}, 'EAP')\n",
      "\n",
      "Keys with features removed:\n",
      "['afforded', 'ascertaining', 'whence', 'This', 'might', 'I', 'however', 'point', '.', ';', 'seemed', 'make', 'circuit', ',', 'dungeon', 'means', 'without', 'uniform', 'dimensions', 'aware', 'fact', 'wall', 'return', 'set', 'perfectly', 'process']\n"
     ]
    }
   ],
   "source": [
    "# create a labeled set of training features. len(all_words)\n",
    "# in other words: all_words = set(word.lower() for passage in modified_corpus for word in passage[0])\n",
    "labeled_data = prepare_data(df)\n",
    "all_data = add_class(labeled_data)\n",
    "    \n",
    "print(\"Keys with features removed:\")\n",
    "print(list(all_data[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle #get a random set of features to use to create negative examples for training.\n",
    "def add_random_negatives(labeled, data_with_classes, percent_negative=1):#allows 0.0-2.0 for percent negative\n",
    "    if(percent_negative > 2 or percent_negative < 0):\n",
    "        percent_negative = 1\n",
    "    all_words = set()  \n",
    "    for passage in labeled:\n",
    "        for word in passage[0]:\n",
    "            all_words.add(word)\n",
    "    all_words = list(all_words)\n",
    "    print(all_words[:5])\n",
    "    all_words_idx = [i for i,_ in enumerate(all_words)]\n",
    "    shuffled_word_idxs = [i for i,_ in enumerate(all_words)]\n",
    "    shuffle(shuffled_word_idxs)# shuffle the indexes so that we can produce reamdom samples\n",
    "    print(shuffled_word_idxs[:5])\n",
    "    print([all_words[shuffled_idx] for shuffled_idx in shuffled_word_idxs[:5]])\n",
    "    # Add some of the shuffled terms as negative examples for each of the data samples.\n",
    "    allw = len(all_words)\n",
    "    idx = 0 # we are going to loop through the shuffled values.\n",
    "    for passage in data_with_classes:\n",
    "        sample = list(passage[0].keys())\n",
    "        j = 0\n",
    "        num_to_add = len(sample) * percent_negative#  add the same number of negative samples as positive. try another model with different ammount of these\n",
    "        while j < num_to_add:\n",
    "            current = all_words[shuffled_word_idxs[idx]]\n",
    "            #print(current)\n",
    "            if current not in sample:\n",
    "                passage[0][current] = False#  add the current term as a negative sample\n",
    "                j = j+1## increment j\n",
    "            idx = idx+1\n",
    "            if idx == allw:\n",
    "                idx = 0 # reset and go around again                \n",
    "    print(data_with_classes[0])\n",
    "    return data_with_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fanlight', 'footman', 'Doctor', 'dreamings', 'Prophet']\n",
      "[12257, 26067, 14504, 16736, 3149]\n",
      "['Virtu', 'Metzengerstein', 'stem', 'Moreover', 'reveries']\n",
      "({'impelled': False, 'irresponsibility': False, 'rules': False, 'fact': True, 'adaption': False, 'make': True, 'respectable': False, 'Guyon': False, 'bot': False, 'noises': False, 'ascertaining': True, \"'Boy\": False, 'red': False, 'Paterson': False, 'steeple': False, 'thy': False, 'Et': False, 'stroked': False, 'sojourn': False, 'obliterated': False, 'condensed': False, 'circled': False, 'traffic': False, 'peaked': False, 'i.e.': False, 'Leipsic': False, 'possibly': False, 'exaggerating': False, 'comer': False, 'Andrée': False, 'churning': False, 'point': True, 'killing': False, ';': True, 'I': True, 'process': True, 'downstairs': False, 'CHARMION': False, 'bygone': False, 'theosophical': False, 'Maine': False, 'functions': False, 'stem': False, 'equal': False, 'aware': True, 'without': True, 'afforded': True, 'Basil': False, 'Showed': False, 'readings': False, 'perfectionate': False, 'facade': False, 'dazzled': False, 'uniform': True, 'circuit': True, 'Metzengerstein': False, 'dimensions': True, 'Albany': False, 'shivered': False, 'printer': False, 'dickey': False, 'Fathers': False, 'dash': False, 'burgomaster': False, 'however': True, 'perfectly': True, 'might': True, 'tacked': False, 'seemed': True, 'swamp': False, 'This': True, 'judging': False, 'set': True, 'eloquence': False, 'rout': False, 'query': False, 'reviving': False, 'heterogeneity': False, 'dungeon': True, 'thaw': False, 'Norwegian': False, 'reveries': False, 'whence': True, 'panelling': False, 'procession': False, 'Moreover': False, 'wall': True, 'sundials': False, 'Let': False, 'Mate': False, ',': True, 'vehicles': False, 'constriction': False, 'hitch': False, 'ignited': False, 'rights': False, 'means': True, 'escaping': False, 'enwrapped': False, 'Virtu': False, 'BE': False, 'return': True, \"wider'n\": False, '.': True}, 'EAP')\n"
     ]
    }
   ],
   "source": [
    "all_data = add_random_negatives(labeled_data, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def seperate_data(prepared_data, cv):# separate training and testing data\n",
    "    shuffle(prepared_data)\n",
    "    cv = CountVectorizer(max_features = 1500)\n",
    "    X = cv.fit_transform(all_data2).toarray()\n",
    "    train_len = math.ceil(len(prepared_data)*.8)\n",
    "    train_data = prepared_data[:train_len]\n",
    "    test_data = prepared_data[train_len:]\n",
    "    test_data_stripped = list(test[0] for test in test_data)\n",
    "    return train_data, test_data, test_data_stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tuberoses': False, 'offered': False, 'quenched': False, \"D'Indaginé\": False, 'next': False, 'proximity': False, 'marchers': False, 'cast': True, 'contrary': True, 'footstool': False, 'collocations': False, 'painful': True, 'horrify': False, 'circumstance': True, 'transacted': False, 'resided': False, 'Tem': False, 'timidly': False, 'Wheeler': False, 'rummaging': False, 'trolleys': False, 'universe': False, 'flowed': False, 'counterfeits': False, 'Oonai': False, 'desk': False, 'declivity': False, 'Alcyone': False, 'salts': False, 'weaken': False, 'mind': True, 'ossi': False, 'retrogression': False, 'Raymond': True, 'monopoly': False, 'wearying': False, 'every': True, 'observant': False, 'unhappy': False, 'aids': False, 'Malign': False, 'told': False, 'contagion': True, 'Trist': False, ';': True, \"'impossibilities\": False, 'Huge': False, 'body': False, 'eerily': False, 'inherit': False, 'foul': True, 'etc': False, 'Corroborates': False, 'stove': False, 'Calcutta': False, 'essence': True, 'Sarnath': False, 'life': True, ':': True, 'papers': False, 'fire': True, 'plicable': False, 'hasp': False, 'Jefferson': False, 'mellowing': False, 'The': True, 'handsom': False, 'shielded': False, 'nest': False, 'tearing': False, 'pure': True, 'clost': False, 'stillness': False, 'donkeys': False, 'apparitions': False, 'rough': True, 'fruition': False, 'atmosphere': True, 'industrial': False, 'Empires': False, 'potter': False, 'pavilions': False, 'cautioned': False, 'patchy': False, 'fades': True, 'repayment': False, 'rune': False, 'served': False, 'meteoric': False, 'spirit': True, 'make': True, 'singin': False, 'proof': True, 'nerve': True, 'Fairies': False, 'musique': False, 'vastnesses': False, 'abuses': False, 'consequent': False, 'towers': False, 'change': True, 'triple': False, 'playing': False, 'hypnotised': False, 'surfaced': False, 'resilience': False, 'cadaverousness': False, 'shrinks': True, 'Théâtre': False, 'forbidden': False, 'sacred': False, 'Aurora': False, 'wigless': False, 'become': True, 'admeasurement': False, 'salary': False, ',': True, 'sinking': False, 'logical': False, 'Gush': False, 'incorporated': True, 'Levantine': False, 'mole': False, 'rudely': True, 'handled': True, 'withers': False, 'considerations': True, '.': True}\n",
      "Most Informative Features\n",
      "                 Raymond = True              MWS : HPL    =    129.4 : 1.0\n",
      "                    West = True              HPL : EAP    =     51.3 : 1.0\n",
      "                    dear = True              MWS : HPL    =     46.0 : 1.0\n",
      "                 towards = True              MWS : HPL    =     44.7 : 1.0\n",
      "                  misery = True              MWS : EAP    =     43.3 : 1.0\n",
      "                sinister = True              HPL : EAP    =     41.1 : 1.0\n",
      "                     Old = True              HPL : EAP    =     40.1 : 1.0\n",
      "                 windows = True              HPL : MWS    =     38.5 : 1.0\n",
      "             endeavoured = True              MWS : EAP    =     36.3 : 1.0\n",
      "                  though = True              HPL : EAP    =     36.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, test_data_stripped = seperate_data(all_data)\n",
    "print(test_data_stripped[0])\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_data)#create Naive Bayes Classifier\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(classify):\n",
    "    preds = [classify.classify(test) for test in test_data_stripped]\n",
    "    #print(preds[0])\n",
    "    #print(test_data[0][1])\n",
    "    accuracy = 0.0\n",
    "    len_preds = len(preds)\n",
    "    for i in range(len_preds):\n",
    "        accuracy += (preds[i] == test_data[i][1])\n",
    "    \n",
    "    accuracy /= len(preds)\n",
    "    print(\"Accuracy:\")\n",
    "    print(accuracy)\n",
    "    print(\"For authors:\")\n",
    "    print(classify.labels())\n",
    "    print(\"Over test data:\")\n",
    "\n",
    "    dftest =  pd.read_csv(\"../input/test.csv\")# read the test data\n",
    "    print(dftest.shape)\n",
    "    print(dftest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.5376756066411239\n",
      "For authors:\n",
      "['MWS', 'HPL', 'EAP']\n",
      "Over test data:\n",
      "(8392, 2)\n",
      "        id                                               text\n",
      "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
      "1  id24541  If a fire wanted fanning, it could readily be ...\n",
      "2  id00134  And when they had broken down the frail door t...\n",
      "3  id27757  While I was thinking how I should possibly man...\n",
      "4  id04081  I am not sure to what limit his knowledge may ...\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code was a baseline example by Joe Doumolin at: https://github.com/JoeDumoulin/CSCD439F17/blob/master/notebooks/Final%20Project/Text%20Processing.ipynb using a Naive Bayes classifier model. The model ended up with about a 55% prediction accuracy. It is now my goal to try to make a model that does better than that. I've modified some of Joe's code to better suit that purpose. I first want to try decreasing/increasing the number of false examples per dataset. I also want to try and change how the model is setup so that the false words in each passage didn't come from the author who wrote that passage.\n",
    "I want to first start by adding double the ammount of negatives to the model and see if that positively or negatively affects the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP', 'HPL', 'MWS']\n",
      "(['This', 'process', ',', 'however', ',', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', ';', 'I', 'might', 'make', 'circuit', ',', 'return', 'point', 'whence', 'I', 'set', ',', 'without', 'aware', 'fact', ';', 'perfectly', 'uniform', 'seemed', 'wall', '.'], 'EAP')\n",
      "({'afforded': True, 'ascertaining': True, 'whence': True, 'This': True, 'might': True, 'I': True, 'however': True, 'point': True, '.': True, ';': True, 'seemed': True, 'make': True, 'circuit': True, ',': True, 'dungeon': True, 'means': True, 'without': True, 'uniform': True, 'dimensions': True, 'aware': True, 'fact': True, 'wall': True, 'return': True, 'set': True, 'perfectly': True, 'process': True}, 'EAP')\n",
      "\n",
      "['obsessions', 'Gresset', 'retread', 'Birch', 'scymetars']\n",
      "[18689, 22898, 13008, 4387, 11557]\n",
      "['Stripes', \"'but\", 'Journal', 'immortal', 'shot']\n",
      "({'afforded': True, 'despises': False, 'doll': False, 'Subterrene': False, 'purplish': False, 'whence': True, 'process': True, 'coach': False, 'Streets': False, 'incredibly': False, 'highest': False, 'devastatingly': False, '.': True, 'shot': False, 'dungeon': True, 'explicitly': False, 'might': True, 'make': True, 'Joe': False, 'swear': False, 'Stripes': False, 'kingdoms': False, 'offices': False, 'disobey': False, 'immortal': False, 'arithmetic': False, ',': True, 'aware': True, 'furthering': False, 'ascertaining': True, 'elses': False, 'fact': True, 'return': True, 'practised': False, 'illness': False, 'set': True, \"'but\": False, 'naivete': False, 'portraiture': False, 'however': True, 'solemn': False, 'This': True, 'California': False, 'renovated': False, 'I': True, 'tantalisingly': False, 'council': False, 'treatise': False, 'point': True, 'Cancale': False, 'admonisher': False, 'addition': False, 'automatics': False, 'seemed': True, 'variety': False, 'entrancing': False, 'circuit': True, 'Aran': False, 'shroud': False, 'superabundant': False, 'amongst': False, 'means': True, 'without': True, 'Ver': False, 'uniform': True, 'twittering': False, 'thickened': False, 'dimensions': True, 'artifice': False, 'Journal': False, 'plaints': False, 'eternities': False, 'dexterously': False, 'wall': True, ';': True, 'trencher': False, 'implements': False, 'perfectly': True}, 'EAP')\n"
     ]
    }
   ],
   "source": [
    "labeled_data = prepare_data(df)\n",
    "prepared = add_class(labeled_data)\n",
    "all_data2 = add_random_negatives(labeled_data, prepared, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cats': False, 'Mostly': False, 'Te': False, 'girdled': False, 'fru': False, 'comings': False, 'kinsmen': False, 'pots': False, 'interference': True, 'Rhone': False, 'wholesale': False, 'suspects': False, 'comprising': False, 'mxther': False, '.': True, 'copiousness': False, 'Arabella': False, 'casks': False, 'excited': True, 'reëntered': False, 'Health': False, 'ce': False, 'followed': True, 'shaggy': False, 'responded': False, 'consummation': False, 'violence': True, 'bounteous': False, 'Survive': False, ',': True, 'indignation': True, 'Grave': False, 'ingratitude': True, 'flight': False, 'public': True, 'renewed': True, 'blackest': True, 'muffling': False, 'Elizabeth': True, 'refuted': False, 'turned': True, 'inanimate': False, 'powerful': True, 'vale': False, 'creations': False, 'appeal': True, 'generous': True, 'murmur': True, 'incidents': False, 'allow': False, 'carried': False, 'approbation': True, 'poor': True, 'Landaff': False, 'Machen': False, 'neutral': False, 'derives': False, 'A': True, 'behoved': False, 'regions': False, 'FOUND': False, 'shuddered': False, 'saound': False, 'edifice': False, 'maintains': False, \"'man\": False, 'simple': True, 'calming': False, 'vapors': False, 'favour': True, 'Justine': True, 'charging': True, 'ideals': False, 'interstices': False, \"'s\": True}\n",
      "Most Informative Features\n",
      "                 Raymond = True              MWS : HPL    =     82.1 : 1.0\n",
      "                      'd = True              HPL : EAP    =     46.1 : 1.0\n",
      "                 towards = True              MWS : HPL    =     43.7 : 1.0\n",
      "                  sister = True              MWS : HPL    =     39.7 : 1.0\n",
      "                  misery = True              MWS : EAP    =     39.6 : 1.0\n",
      "                    grey = True              HPL : EAP    =     37.7 : 1.0\n",
      "                 despite = True              HPL : EAP    =     35.8 : 1.0\n",
      "                    West = True              HPL : EAP    =     33.3 : 1.0\n",
      "                pleasure = True              MWS : HPL    =     32.9 : 1.0\n",
      "                     Old = True              HPL : MWS    =     32.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#train_data, test_data, test_data_stripped, count_v = seperate_data(all_data2)\n",
    "cv = CountVectorizer(max_features = 1500)#not using for now\n",
    "shuffle(all_data2) \n",
    "#X = cv.fit_transform(all_data2).toarray()\n",
    "#y = all_data2[:, 2].values\n",
    "\n",
    "train_len = math.ceil(len(all_data2)*.8)\n",
    "train_data = all_data2[:train_len]\n",
    "test_data = all_data2[train_len:]\n",
    "test_data_stripped = list(test[0] for test in test_data)\n",
    "    #return train_data, test_data, test_data_stripped\n",
    "print(test_data_stripped[0])\n",
    "\n",
    "classifier2 = nltk.NaiveBayesClassifier.train(train_data)#create Naive Bayes Classifier\n",
    "classifier2.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.5149425287356322\n",
      "For authors:\n",
      "['HPL', 'MWS', 'EAP']\n",
      "Over test data:\n",
      "(8392, 2)\n",
      "        id                                               text\n",
      "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
      "1  id24541  If a fire wanted fanning, it could readily be ...\n",
      "2  id00134  And when they had broken down the frail door t...\n",
      "3  id27757  While I was thinking how I should possibly man...\n",
      "4  id04081  I am not sure to what limit his knowledge may ...\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(classifier2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this a few times it seems that doubling the ammount of negative examples within the model actually decreased the accuracy of the model significantly. The question that follows is, will decreasing the amount of negatives increase the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP', 'HPL', 'MWS']\n",
      "(['This', 'process', ',', 'however', ',', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', ';', 'I', 'might', 'make', 'circuit', ',', 'return', 'point', 'whence', 'I', 'set', ',', 'without', 'aware', 'fact', ';', 'perfectly', 'uniform', 'seemed', 'wall', '.'], 'EAP')\n",
      "({'afforded': True, 'ascertaining': True, 'whence': True, 'This': True, 'might': True, 'I': True, 'however': True, 'point': True, '.': True, ';': True, 'seemed': True, 'make': True, 'circuit': True, ',': True, 'dungeon': True, 'means': True, 'without': True, 'uniform': True, 'dimensions': True, 'aware': True, 'fact': True, 'wall': True, 'return': True, 'set': True, 'perfectly': True, 'process': True}, 'EAP')\n",
      "\n",
      "['obsessions', 'Gresset', 'retread', 'Birch', 'scymetars']\n",
      "[22460, 19047, 7791, 26790, 10283]\n",
      "['fruits', 'peopled', 'Nubium', 'rumble', 'citations']\n",
      "({'afforded': True, 'Wisely': False, 'fades': False, 'despondence': False, 'citations': False, 'uniform': True, 'whence': True, 'Nubium': False, '.': True, 'dungeon': True, 'requisition': False, 'might': True, 'disobey': False, 'Grace': False, 'peopled': False, 'senility': False, 'widening': False, ',': True, 'aware': True, 'outspeed': False, 'ascertaining': True, 'return': True, 'whatsoever': False, 'set': True, 'process': True, 'perfectly': True, 'fruits': False, 'however': True, 'This': True, 'I': True, 'transmitted': False, 'point': True, 'purgatory': False, 'seemed': True, 'rumble': False, 'circuit': True, 'blieve': False, 'gun': False, 'means': True, 'without': True, 'domesticate': False, 'dimensions': True, 'fact': True, 'wall': True, ';': True, 'make': True}, 'EAP')\n",
      "{'saw': False, '?': True, 'But': True, \"'what\": False, 'pestilences': False, 'joy': True, 'raptures': False, 'sorrow': True, 'past': True, 'mulattoes': False, 'memory': True, 'dancing': False, 'present': True}\n",
      "Most Informative Features\n",
      "                 Raymond = True              MWS : HPL    =    133.9 : 1.0\n",
      "               happiness = True              MWS : HPL    =     48.7 : 1.0\n",
      "                    dear = True              MWS : HPL    =     48.7 : 1.0\n",
      "               monstrous = True              HPL : EAP    =     37.3 : 1.0\n",
      "               Elizabeth = True              MWS : HPL    =     36.6 : 1.0\n",
      "                sinister = True              HPL : EAP    =     36.4 : 1.0\n",
      "                     Old = True              HPL : EAP    =     35.5 : 1.0\n",
      "                    grey = True              HPL : EAP    =     34.5 : 1.0\n",
      "                      'd = True              HPL : EAP    =     34.5 : 1.0\n",
      "                  sister = True              MWS : EAP    =     33.0 : 1.0\n",
      "Accuracy:\n",
      "0.6086845466155811\n",
      "For authors:\n",
      "['HPL', 'MWS', 'EAP']\n",
      "Over test data:\n",
      "(8392, 2)\n",
      "        id                                               text\n",
      "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
      "1  id24541  If a fire wanted fanning, it could readily be ...\n",
      "2  id00134  And when they had broken down the frail door t...\n",
      "3  id27757  While I was thinking how I should possibly man...\n",
      "4  id04081  I am not sure to what limit his knowledge may ...\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data(df)\n",
    "prepared = add_class(data)\n",
    "all_data3 = add_random_negatives(data, prepared, 0.75)#each passage gets 75% of its size in negative values\n",
    "shuffle(all_data3) \n",
    "\n",
    "train_len = math.ceil(len(all_data3)*.8)\n",
    "train_data = all_data3[:train_len]\n",
    "test_data = all_data3[train_len:]\n",
    "test_data_stripped = list(test[0] for test in test_data)\n",
    "print(test_data_stripped[0])\n",
    "\n",
    "classifier3 = nltk.NaiveBayesClassifier.train(train_data)#create Naive Bayes Classifier\n",
    "classifier3.show_most_informative_features()\n",
    "print_accuracy(classifier3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of adjusting the number of negative features\n",
    "While the Bayesian model didn't produce highly accurate results, I was able to make minor advancements upon the base model I started with to end up with a more accurate predictions. My best guess as to why reducing the ammount of negatives helped the model would be that it mayhave reduced the chance of similar words appearing as negatives in one of the passages. A future attempt might be to try and create a large dictionary of words by each given author and ensure none of the negatives for a given passage are ever used by that author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP', 'HPL', 'MWS']\n",
      "(['This', 'process', ',', 'however', ',', 'afforded', 'means', 'ascertaining', 'dimensions', 'dungeon', ';', 'I', 'might', 'make', 'circuit', ',', 'return', 'point', 'whence', 'I', 'set', ',', 'without', 'aware', 'fact', ';', 'perfectly', 'uniform', 'seemed', 'wall', '.'], 'EAP')\n",
      "({'afforded': True, 'ascertaining': True, 'whence': True, 'This': True, 'might': True, 'I': True, 'however': True, 'point': True, '.': True, ';': True, 'seemed': True, 'make': True, 'circuit': True, ',': True, 'dungeon': True, 'means': True, 'without': True, 'uniform': True, 'dimensions': True, 'aware': True, 'fact': True, 'wall': True, 'return': True, 'set': True, 'perfectly': True, 'process': True}, 'EAP')\n",
      "\n",
      "['obsessions', 'Gresset', 'retread', 'Birch', 'scymetars']\n",
      "[6800, 14378, 14049, 22975, 22001]\n",
      "[\"'although\", 'discharge', 'declin', 'devotions', 'armour']\n",
      "({'afforded': True, 'walls': False, 'experience': False, '..': False, 'whence': True, 'process': True, 'mutable': False, '.': True, 'unlined': False, 'dungeon': True, 'wretchedly': False, 'might': True, 'honours': False, \"'although\": False, ',': True, 'aware': True, 'ascertaining': True, 'devotions': False, 'return': True, 'set': True, \"'anything\": False, 'however': True, 'This': True, 'discharge': False, 'I': True, 'prosecution': False, 'commended': False, 'point': True, 'armour': False, 'receiving': False, 'declin': False, 'theme': False, 'analogy': False, 'seemed': True, 'make': True, 'circuit': True, 'impertinences': False, 'signboards': False, 'means': True, 'without': True, 'uniform': True, 'dimensions': True, 'fact': True, 'wall': True, ';': True, 'perfectly': True}, 'EAP')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-6a144626a705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0msubmission_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"m1submission.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0moutput_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-6a144626a705>\u001b[0m in \u001b[0;36moutput_data\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#train_data = all_data[:train_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#x_data = all_data[train_len:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#Print to cv file\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def output_data(model):\n",
    "    test = pd.read_csv(\"../input/train.csv\")\n",
    "    data = prepare_data(test)\n",
    "    prepared = add_class(data)\n",
    "    all_data = add_random_negatives(data, prepared, 0.75)\n",
    "    #test[\"length\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "    #corpus = test['text'].tolist()# Read the text of the training examples  \n",
    "    #labels = test['author'].tolist()\n",
    "    #labeled_data = prepare_data(df)\n",
    "    #eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    #test_corpus_text = []\n",
    "    #all_data = add_class(labeled_data)\n",
    "    #for i in range(0, test.shape[0]):\n",
    "    #    corpus = test[\"text\"][i]\n",
    "    #    corpus = corpus.lower()\n",
    "    #    corpus = corpus.split()\n",
    "    #    ps = PorterStemmer()\n",
    "    #    corpus = [ps.stem(word) for word in corpus if not word in eng_stopwords]\n",
    "    #    corpus = ' '.join(corpus)\n",
    "    #    test_corpus_text.append(corpus)\n",
    "    #train_len = math.ceil(len(all_data))\n",
    "    #train_data = all_data[:train_len]\n",
    "    #x_data = all_data[train_len:]\n",
    "    print(all_data.shape)\n",
    "    dict(zip(all_data))\n",
    "    print(type(all_data))\n",
    "    y_predicted = model.prob_classify(all_data)\n",
    "    #X_test_output = cv.transform(test_corpus_text).toarray()\n",
    "    #y_prob_output = classifier.predict_proba(X_test_output)\n",
    "    submission_df = pd.DataFrame(y_predicted,index=test['id'],columns=['EAP','HPL','MWS'])\n",
    "    submission_df.to_csv(\"m1submission.csv\")\n",
    "\n",
    "output_data(classifier3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
